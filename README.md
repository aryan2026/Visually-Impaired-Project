# EPICS 

Designed for the blind and low vision community, this research project harnesses
the power of AI to describe people, text, and objects. This project brings together
the power of AI to deliver an intelligent system designed to help you navigate your
day. Point your phone’s camera, select a channel, and hear a description of what the
AI has recognized around you.

![image](https://user-images.githubusercontent.com/98420946/187473004-8e2515aa-267c-48d9-9493-3cbec2da56dd.png)



With its intelligent system, just hold up your camera and hear information about the
world around you. Our system will speak short text as it appears in front of the
camera, provide audio guidance to capture a printed page, and recognize and narrate
the text.


● Recognize and locate the faces of people you’re with.


● Reads text quickly and gets audio guidance to capture full documents.


Our project will be an extended work of real-time object detection. We will
implement real-time object detection using COCO API, which detects the object
on live video stream and converts the objects to speech, and give a gist of where
the object is.

## Objective

The project's aim is to help visually impaired people by using real-time object
detection in a live video stream, converting the detected object to speech, and
describing the position of the object. Apart from this, we will also implement the
image-to-speech conversion. Mainly, Deep Learning will be used for
Implementation. We will create a user interface to merge both modules. We will
discuss the progress made leading up to the current development scene and possible
future enhancements that can serve as motivation for further work.

